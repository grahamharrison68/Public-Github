{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/03.%20Feature%20Engineering/03.%20Feature%20Engineering.ipynb\n",
    "\n",
    "https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/04.%20Model%20Training/06.%20MT%20-%20Random%20Forest.ipynb\n",
    "\n",
    "https://towardsdatascience.com/text-classification-in-python-dd95d264c802\n",
    "\n",
    "https://github.com/miguelfzafra/Latest-News-Classifier/tree/master/0.%20Latest%20News%20Classifier/04.%20Model%20Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the data\n",
    "2. Test train split\n",
    "3. use TFIFD to add in the new features\n",
    "4. Use an algorithm to make some predictions\n",
    "5. Assess the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Transformed Review Text</th>\n",
       "      <th>Transformed Review Text_wordcloud</th>\n",
       "      <th>Main Emotion</th>\n",
       "      <th>Main Emotion Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850</td>\n",
       "      <td>64</td>\n",
       "      <td>A really cute top</td>\n",
       "      <td>I am normally an xs or s in tanks, so ordered ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>I am normally an xs or s in tanks, so ordered ...</td>\n",
       "      <td>normally xs tanks ordered size looks pretty mu...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.588694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>829</td>\n",
       "      <td>30</td>\n",
       "      <td>Great shirt</td>\n",
       "      <td>I ordered this shirt last week, and i wore it ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>I ordered this shirt last week, and i wore it ...</td>\n",
       "      <td>ordered shirt last week wore soon arrived beau...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.806345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1008</td>\n",
       "      <td>33</td>\n",
       "      <td>Great skirt</td>\n",
       "      <td>Love the material -- it is a thick cotton, but...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>Love the material -- it is a thick cotton, but...</td>\n",
       "      <td>love material thick cotton structured enough t...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.626453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>872</td>\n",
       "      <td>37</td>\n",
       "      <td>Lovely top</td>\n",
       "      <td>This is a beautiful, simple, yet feminine top....</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>This is a beautiful, simple, yet feminine top....</td>\n",
       "      <td>beautiful simple yet feminine top little tight...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.693493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1056</td>\n",
       "      <td>26</td>\n",
       "      <td>Soft but fit oddly</td>\n",
       "      <td>I wanted to love these, but the zipper is extr...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>I wanted to love these, but the zipper is extr...</td>\n",
       "      <td>wanted love zipper extremely short flattering ...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.600855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age               Title  \\\n",
       "0          850   64   A really cute top   \n",
       "1          829   30         Great shirt   \n",
       "2         1008   33         Great skirt   \n",
       "3          872   37          Lovely top   \n",
       "4         1056   26  Soft but fit oddly   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I am normally an xs or s in tanks, so ordered ...       4                1   \n",
       "1  I ordered this shirt last week, and i wore it ...       5                1   \n",
       "2  Love the material -- it is a thick cotton, but...       5                1   \n",
       "3  This is a beautiful, simple, yet feminine top....       5                1   \n",
       "4  I wanted to love these, but the zipper is extr...       3                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                       31  General Petite            Tops    Blouses   \n",
       "1                        0         General            Tops    Blouses   \n",
       "2                        4         General         Bottoms     Skirts   \n",
       "3                        0         General            Tops      Knits   \n",
       "4                        0  General Petite         Bottoms      Pants   \n",
       "\n",
       "                             Transformed Review Text  \\\n",
       "0  I am normally an xs or s in tanks, so ordered ...   \n",
       "1  I ordered this shirt last week, and i wore it ...   \n",
       "2  Love the material -- it is a thick cotton, but...   \n",
       "3  This is a beautiful, simple, yet feminine top....   \n",
       "4  I wanted to love these, but the zipper is extr...   \n",
       "\n",
       "                   Transformed Review Text_wordcloud Main Emotion  \\\n",
       "0  normally xs tanks ordered size looks pretty mu...          Joy   \n",
       "1  ordered shirt last week wore soon arrived beau...          Joy   \n",
       "2  love material thick cotton structured enough t...          Joy   \n",
       "3  beautiful simple yet feminine top little tight...          Joy   \n",
       "4  wanted love zipper extremely short flattering ...          Joy   \n",
       "\n",
       "   Main Emotion Score  \n",
       "0            0.588694  \n",
       "1            0.806345  \n",
       "2            0.626453  \n",
       "3            0.693493  \n",
       "4            0.600855  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews = pd.read_excel(\"data/out/Womens Clothing E-Commerce Reviews with Emotions.xlsx\")\n",
    "df_reviews.dropna(inplace=True) \n",
    "df_reviews.reset_index(drop=True, inplace=True)\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joy        390\n",
       "Sadness     33\n",
       "Fear         9\n",
       "Anger        4\n",
       "Name: Main Emotion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews['Main Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Transformed Review Text</th>\n",
       "      <th>Transformed Review Text_wordcloud</th>\n",
       "      <th>Main Emotion</th>\n",
       "      <th>Main Emotion Score</th>\n",
       "      <th>Main Emotion Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850</td>\n",
       "      <td>64</td>\n",
       "      <td>A really cute top</td>\n",
       "      <td>I am normally an xs or s in tanks, so ordered ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>I am normally an xs or s in tanks, so ordered ...</td>\n",
       "      <td>normally xs tanks ordered size looks pretty mu...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.588694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>829</td>\n",
       "      <td>30</td>\n",
       "      <td>Great shirt</td>\n",
       "      <td>I ordered this shirt last week, and i wore it ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>I ordered this shirt last week, and i wore it ...</td>\n",
       "      <td>ordered shirt last week wore soon arrived beau...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.806345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1008</td>\n",
       "      <td>33</td>\n",
       "      <td>Great skirt</td>\n",
       "      <td>Love the material -- it is a thick cotton, but...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>Love the material -- it is a thick cotton, but...</td>\n",
       "      <td>love material thick cotton structured enough t...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.626453</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>872</td>\n",
       "      <td>37</td>\n",
       "      <td>Lovely top</td>\n",
       "      <td>This is a beautiful, simple, yet feminine top....</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>This is a beautiful, simple, yet feminine top....</td>\n",
       "      <td>beautiful simple yet feminine top little tight...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.693493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1056</td>\n",
       "      <td>26</td>\n",
       "      <td>Soft but fit oddly</td>\n",
       "      <td>I wanted to love these, but the zipper is extr...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>I wanted to love these, but the zipper is extr...</td>\n",
       "      <td>wanted love zipper extremely short flattering ...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0.600855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age               Title  \\\n",
       "0          850   64   A really cute top   \n",
       "1          829   30         Great shirt   \n",
       "2         1008   33         Great skirt   \n",
       "3          872   37          Lovely top   \n",
       "4         1056   26  Soft but fit oddly   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I am normally an xs or s in tanks, so ordered ...       4                1   \n",
       "1  I ordered this shirt last week, and i wore it ...       5                1   \n",
       "2  Love the material -- it is a thick cotton, but...       5                1   \n",
       "3  This is a beautiful, simple, yet feminine top....       5                1   \n",
       "4  I wanted to love these, but the zipper is extr...       3                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                       31  General Petite            Tops    Blouses   \n",
       "1                        0         General            Tops    Blouses   \n",
       "2                        4         General         Bottoms     Skirts   \n",
       "3                        0         General            Tops      Knits   \n",
       "4                        0  General Petite         Bottoms      Pants   \n",
       "\n",
       "                             Transformed Review Text  \\\n",
       "0  I am normally an xs or s in tanks, so ordered ...   \n",
       "1  I ordered this shirt last week, and i wore it ...   \n",
       "2  Love the material -- it is a thick cotton, but...   \n",
       "3  This is a beautiful, simple, yet feminine top....   \n",
       "4  I wanted to love these, but the zipper is extr...   \n",
       "\n",
       "                   Transformed Review Text_wordcloud Main Emotion  \\\n",
       "0  normally xs tanks ordered size looks pretty mu...          Joy   \n",
       "1  ordered shirt last week wore soon arrived beau...          Joy   \n",
       "2  love material thick cotton structured enough t...          Joy   \n",
       "3  beautiful simple yet feminine top little tight...          Joy   \n",
       "4  wanted love zipper extremely short flattering ...          Joy   \n",
       "\n",
       "   Main Emotion Score  Main Emotion Code  \n",
       "0            0.588694                  0  \n",
       "1            0.806345                  0  \n",
       "2            0.626453                  0  \n",
       "3            0.693493                  0  \n",
       "4            0.600855                  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_map = {\n",
    "    'Joy': 0,\n",
    "    'Sadness': 1,\n",
    "    'Fear': 2,\n",
    "    'Anger': 3\n",
    "}\n",
    "\n",
    "df_reviews['Main Emotion Code'] = df_reviews['Main Emotion']\n",
    "df_reviews = df_reviews.replace({'Main Emotion Code': emotion_map})\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 436 entries, 0 to 435\n",
      "Data columns (total 15 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Clothing ID                        436 non-null    int64  \n",
      " 1   Age                                436 non-null    int64  \n",
      " 2   Title                              436 non-null    object \n",
      " 3   Review Text                        436 non-null    object \n",
      " 4   Rating                             436 non-null    int64  \n",
      " 5   Recommended IND                    436 non-null    int64  \n",
      " 6   Positive Feedback Count            436 non-null    int64  \n",
      " 7   Division Name                      436 non-null    object \n",
      " 8   Department Name                    436 non-null    object \n",
      " 9   Class Name                         436 non-null    object \n",
      " 10  Transformed Review Text            436 non-null    object \n",
      " 11  Transformed Review Text_wordcloud  436 non-null    object \n",
      " 12  Main Emotion                       436 non-null    object \n",
      " 13  Main Emotion Score                 436 non-null    float64\n",
      " 14  Main Emotion Code                  436 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(8)\n",
      "memory usage: 51.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(encoding='utf-8', ngram_range=ngram_range, stop_words=None, lowercase=False, max_df=max_df, min_df=min_df, max_features=max_features, norm='l2', sublinear_tf=True)\n",
    "\n",
    "X = df_reviews['Transformed Review Text_wordcloud']\n",
    "y = df_reviews['Main Emotion Code']\n",
    "\n",
    "tfid_features = tfidf_vectorizer.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlated_ngrams(emotion_code):\n",
    "\n",
    "    features_chi2 = chi2(tfid_features, y == emotion_code) # For the current emotion, use chi2 stats to evaluate the highest correlations with the target\n",
    "    indices = np.argsort(features_chi2[0])[::-1] # np.argsort returns the indices that would sort the array ascending, [::-1] makes it descending with the highest correlation in first place\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices] # get_feature_names uses the sorted indices to order the features most to least correlated with the target\n",
    "    \n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1] # Selects the feature names that are single words\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2] # Selects the feature names that are two words\n",
    "    \n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger:\n",
      "Most correleated unigrams: body, felt, wash, small, looking, retailer, unfortunately, could, detail, put, usual, wardrobe, waist, away, way, tank, maybe, around, made, shoulders\n",
      "Most correleated bigrams: fit perfectly, true size\n",
      "\n",
      "Fear:\n",
      "Most correleated unigrams: area, yet, normal, full, stunning, pull, took, retailer, works, xxs, overall, absolutely, medium, came, either, piece, first, already, might, body\n",
      "Most correleated bigrams: well made, feel like\n",
      "\n",
      "Joy:\n",
      "Most correleated unigrams: return, retailer, tight, order, area, sizes, two, pull, put, bust, however, runs, body, another, away, perfect, maybe, might, full, also\n",
      "Most correleated bigrams: love dress, cannot wait\n",
      "\n",
      "Sadness:\n",
      "Most correleated unigrams: return, order, tight, another, two, decided, also, pink, would, put, sizes, bust, front, away, however, maybe, perfect, around, cut, got\n",
      "Most correleated bigrams: true size, love dress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion, emotion_code in sorted(emotion_map.items()):\n",
    "    unigrams, bigrams = get_correlated_ngrams(emotion_code)\n",
    "    \n",
    "    print(f\"{emotion}:\")\n",
    "    print(f\"Most correleated unigrams: {', '.join(unigrams[:20])}\")\n",
    "    print(f\"Most correleated bigrams: {', '.join(bigrams[:2])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger:\n",
      "Most correleated unigrams: body, felt, wash, small, looking, retailer, unfortunately, could, detail, put, usual, wardrobe, waist, away, way, tank, maybe, around, made, shoulders\n",
      "Most correleated bigrams: fit perfectly, true size\n",
      "\n",
      "Fear:\n",
      "Most correleated unigrams: area, yet, normal, full, stunning, pull, took, retailer, works, xxs, overall, absolutely, medium, came, either, piece, first, already, might, body\n",
      "Most correleated bigrams: well made, feel like\n",
      "\n",
      "Joy:\n",
      "Most correleated unigrams: return, retailer, tight, order, area, sizes, two, pull, put, bust, however, runs, body, another, away, perfect, maybe, might, full, also\n",
      "Most correleated bigrams: love dress, cannot wait\n",
      "\n",
      "Sadness:\n",
      "Most correleated unigrams: return, order, tight, another, two, decided, also, pink, would, put, sizes, bust, front, away, however, maybe, perfect, around, cut, got\n",
      "Most correleated bigrams: true size, love dress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for emotion, emotion_code in sorted(emotion_map.items()):\n",
    "    features_chi2 = chi2(tfid_features, y == emotion_code) # For the current emotion, use chi2 stats to evaluate the highest correlations with the target\n",
    "    indices = np.argsort(features_chi2[0])[::-1] # np.argsort returns the indices that would sort the array ascending, [::-1] makes it descending with the highest correlation in first place\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices] # get_feature_names uses the sorted indices to order the features most to least correlated with the target\n",
    "    \n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1] # Selects the feature names that are single words\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2] # Selects the feature names that are two words\n",
    "    \n",
    "    print(f\"{emotion}:\")\n",
    "    print(f\"Most correleated unigrams: {', '.join(unigrams[:20])}\")\n",
    "    print(f\"Most correleated bigrams: {', '.join(bigrams[:2])}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Anger' category:\n",
      "  . Most correlated unigrams:\n",
      ". looking\n",
      ". small\n",
      ". wash\n",
      ". felt\n",
      ". body\n",
      "  . Most correlated bigrams:\n",
      ". true size\n",
      ". fit perfectly\n",
      "\n",
      "# 'Fear' category:\n",
      "  . Most correlated unigrams:\n",
      ". stunning\n",
      ". full\n",
      ". normal\n",
      ". yet\n",
      ". area\n",
      "  . Most correlated bigrams:\n",
      ". feel like\n",
      ". well made\n",
      "\n",
      "# 'Joy' category:\n",
      "  . Most correlated unigrams:\n",
      ". area\n",
      ". order\n",
      ". tight\n",
      ". retailer\n",
      ". return\n",
      "  . Most correlated bigrams:\n",
      ". cannot wait\n",
      ". love dress\n",
      "\n",
      "# 'Sadness' category:\n",
      "  . Most correlated unigrams:\n",
      ". two\n",
      ". another\n",
      ". tight\n",
      ". order\n",
      ". return\n",
      "  . Most correlated bigrams:\n",
      ". love dress\n",
      ". true size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for emotion, emotion_code in sorted(emotion_map.items()):\n",
    "    features_chi2 = chi2(tfid_features, y == emotion_code) # For the current emotion, use chi2 stats to evaluate the highest correlations with the target\n",
    "    indices = np.argsort(features_chi2[0]) # np.argsort returns the indices that would sort the array !!!IN LOWEST TO HIGHEST ORDER!!!\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices] # get_feature_names uses the sorted indices to order the features most to least correlated with the target\n",
    "    \n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1] # Selects the feature names that are single words\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2] # Selects the feature names that are two words\n",
    "\n",
    "    #print(f\"{emotion}:\")\n",
    "    #print(f\"Most correleated unigrame: {', '.join(unigrams[-5:])}\")\n",
    "    print(\"# '{}' category:\".format(emotion))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_reviews['Transformed Review Text_wordcloud'], \n",
    "                                                    df_reviews['Main Emotion Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_reviews['Transformed Review Text_wordcloud']\n",
    "y_train = df_reviews['Main Emotion Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237496</td>\n",
       "      <td>0.254225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3         4    5    6         7         8    9    ...  \\\n",
       "0    0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.202146  0.000000  0.0  ...   \n",
       "1    0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "2    0.0  0.0  0.0  0.0  0.189198  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "3    0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "4    0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "..   ...  ...  ...  ...       ...  ...  ...       ...       ...  ...  ...   \n",
       "431  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "432  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.237496  0.254225  0.0  ...   \n",
       "433  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "434  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "435  0.0  0.0  0.0  0.0  0.224784  0.0  0.0  0.000000  0.000000  0.0  ...   \n",
       "\n",
       "          263       264  265       266  267       268       269  270  271  \\\n",
       "0    0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.180455  0.0  0.0   \n",
       "1    0.247227  0.000000  0.0  0.000000  0.0  0.170473  0.000000  0.0  0.0   \n",
       "2    0.000000  0.000000  0.0  0.000000  0.0  0.141528  0.000000  0.0  0.0   \n",
       "3    0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "4    0.000000  0.305821  0.0  0.000000  0.0  0.236998  0.000000  0.0  0.0   \n",
       "..        ...       ...  ...       ...  ...       ...       ...  ...  ...   \n",
       "431  0.000000  0.000000  0.0  0.000000  0.0  0.176293  0.000000  0.0  0.0   \n",
       "432  0.000000  0.000000  0.0  0.210165  0.0  0.000000  0.212011  0.0  0.0   \n",
       "433  0.000000  0.000000  0.0  0.000000  0.0  0.149616  0.000000  0.0  0.0   \n",
       "434  0.197121  0.000000  0.0  0.000000  0.0  0.135923  0.000000  0.0  0.0   \n",
       "435  0.000000  0.216976  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "          272  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.366034  \n",
       "4    0.000000  \n",
       "..        ...  \n",
       "431  0.000000  \n",
       "432  0.267200  \n",
       "433  0.000000  \n",
       "434  0.000000  \n",
       "435  0.000000  \n",
       "\n",
       "[436 rows x 273 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_train = tfidf.fit_transform(X_train).toarray()\n",
    "#labels_train = y_train\n",
    "#print(features_train.shape)\n",
    "\n",
    "#features_test = tfidf.transform(X_test).toarray()\n",
    "#labels_test = y_test\n",
    "#print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GHARRI~1\\AppData\\Local\\Temp/ipykernel_2220/3100938419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_chi2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "features_chi2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Anger' category:\n",
      "  . Most correlated unigrams:\n",
      ". looking\n",
      ". small\n",
      ". wash\n",
      ". felt\n",
      ". body\n",
      "  . Most correlated bigrams:\n",
      ". true size\n",
      ". fit perfectly\n",
      "\n",
      "# 'Fear' category:\n",
      "  . Most correlated unigrams:\n",
      ". stunning\n",
      ". full\n",
      ". normal\n",
      ". yet\n",
      ". area\n",
      "  . Most correlated bigrams:\n",
      ". feel like\n",
      ". well made\n",
      "\n",
      "# 'Joy' category:\n",
      "  . Most correlated unigrams:\n",
      ". area\n",
      ". order\n",
      ". tight\n",
      ". retailer\n",
      ". return\n",
      "  . Most correlated bigrams:\n",
      ". cannot wait\n",
      ". love dress\n",
      "\n",
      "# 'Sadness' category:\n",
      "  . Most correlated unigrams:\n",
      ". two\n",
      ". another\n",
      ". tight\n",
      ". order\n",
      ". return\n",
      "  . Most correlated bigrams:\n",
      ". love dress\n",
      ". true size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(emotion_map.items()):\n",
    "    features_chi2 = chi2(tfid_features, y == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfifd_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df_reviews.iloc[:,[1,4,5,6]], pd.DataFrame(tfifd_features)], axis=1)\n",
    "y = df_reviews[\"Main Emotion Code\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 244)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfifd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 273)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfifd_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 436 is different from 370)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GHARRI~1\\AppData\\Local\\Temp/ipykernel_12548/3122022272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mProduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mProduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mfeatures_chi2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfifd_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcategory_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#    indices = np.argsort(features_chi2[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mobserved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# n_classes * n_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mfeature_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 436 is different from 370)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(emotion_map.items()):\n",
    "    print(Product, category_id)\n",
    "    features_chi2 = chi2(tfifd_features, labels_train == category_id)\n",
    "#    indices = np.argsort(features_chi2[0])\n",
    "#    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "#    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#    print(\"# '{}' category:\".format(Product))\n",
    "#    print(\"\\nMost correlated unigrams:\\n{}\".format('\\n'.join(unigrams[-15:])))\n",
    "#    print(\"\\nMost correlated bigrams:\\n{}\".format('\\n'.join(bigrams[-5:])))\n",
    "#    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 436 is different from 348)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GHARRI~1\\AppData\\Local\\Temp/ipykernel_12548/1485634658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mProduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfeatures_chi2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfifd_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcategory_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_chi2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mobserved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# n_classes * n_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mfeature_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 436 is different from 348)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(emotion_map.items()):\n",
    "    features_chi2 = chi2(tfifd_features, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#    trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"\\nMost correlated unigrams:\\n{}\".format('\\n'.join(unigrams[-15:])))\n",
    "    print(\"\\nMost correlated bigrams:\\n{}\".format('\\n'.join(bigrams[-5:])))\n",
    "#    print(\"\\nMost correlated trigrams:\\n{}\".format('\\n'.join(trigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get rid of full stops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
