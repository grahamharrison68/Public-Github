{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Tuple\n",
    "from statistics import mode\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "RANDOM_STATE : int = 42\n",
    "N_SAMPLES : int = 10000\n",
    "N_FEATURES : int = 25\n",
    "N_CLASSES : int = 3\n",
    "N_CLUSTERS_PER_CLASS : int = 2\n",
    "    \n",
    "FEATURE_NAME_PREFIX : str = \"Feature\"\n",
    "TARGET_NAME : str = \"Target\"\n",
    "    \n",
    "N_SPLITS : int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification_dataframe(n_samples : int = 10000, n_features : int = 25, n_classes : int = 2, n_clusters_per_class : int = 2, feature_name_prefix : str = \"Feature\", target_name : str = \"Target\", random_state : int = 42) -> pd.DataFrame:\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative = n_classes * n_clusters_per_class, random_state=RANDOM_STATE)\n",
    "\n",
    "    feature_names = [feature_name_prefix + \" \" + str(v) for v in np.arange(1, N_FEATURES+1)]\n",
    "    return pd.concat([pd.DataFrame(X, columns=feature_names), pd.DataFrame(y, columns=[target_name])], axis=1)\n",
    "\n",
    "df_data = make_classification_dataframe(n_samples=N_SAMPLES, n_features=N_FEATURES, n_classes=N_CLASSES, n_clusters_per_class=N_CLUSTERS_PER_CLASS, feature_name_prefix=FEATURE_NAME_PREFIX, target_name=TARGET_NAME, random_state=RANDOM_STATE)\n",
    "\n",
    "X = df_data.drop([TARGET_NAME], axis=1).to_numpy()\n",
    "y = df_data[TARGET_NAME].to_numpy()\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict(model, kfold : KFold, X : np.array, y : np.array) -> Tuple[np.array, np.array, np.array]:\n",
    "\n",
    "    model_ = cp.deepcopy(model)\n",
    "    \n",
    "    no_classes = len(np.unique(y))\n",
    "    \n",
    "    actual_classes = np.empty([0], dtype=int)\n",
    "    predicted_classes = np.empty([0], dtype=int)\n",
    "    predicted_proba = np.empty([0, no_classes]) \n",
    "\n",
    "    for train_ndx, test_ndx in kfold.split(X):\n",
    "\n",
    "        train_X, train_y, test_X, test_y = X[train_ndx], y[train_ndx], X[test_ndx], y[test_ndx]\n",
    "\n",
    "        actual_classes = np.append(actual_classes, test_y)\n",
    "\n",
    "        model_.fit(train_X, train_y)\n",
    "        predicted_classes = np.append(predicted_classes, model_.predict(test_X))\n",
    "\n",
    "        try:\n",
    "            predicted_proba = np.append(predicted_proba, model_.predict_proba(test_X), axis=0)\n",
    "        except:\n",
    "            predicted_proba = np.append(predicted_proba, np.zeros((len(test_X), no_classes), dtype=float), axis=0)\n",
    "\n",
    "    return actual_classes, predicted_classes, predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 312 ms\n",
      "Accuracy of Logistic Regression: 0.6821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 2, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "kfold = KFold(n_splits=N_SPLITS, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "%time actual, lr_predicted, lr_predicted_proba = cross_val_predict(lr, kfold, X, y)\n",
    "print(f\"Accuracy of Logistic Regression: {accuracy_score(actual, lr_predicted)}\")\n",
    "lr_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.4 s\n",
      "Accuracy of Random Forrest: 0.8742\n",
      "Wall time: 20.9 s\n",
      "Accuracy of XG Boost: 0.8838\n",
      "Wall time: 8.04 s\n",
      "Accuracy of Extra Random Trees: 0.8754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2, 0, 0, ..., 0, 2, 1]),\n",
       " array([2, 0, 2, ..., 0, 2, 1], dtype=int64),\n",
       " array([2, 0, 0, ..., 0, 2, 1])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = dict()\n",
    "classifiers[\"Random Forrest\"] = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "classifiers[\"XG Boost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)\n",
    "classifiers[\"Extra Random Trees\"] = ExtraTreesClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "predictions = [None] * len(classifiers)\n",
    "predicted_probas = [None] * len(classifiers)\n",
    "\n",
    "for i, (name, classifier) in enumerate(classifiers.items()):\n",
    "    %time actual, predictions[i], predicted_probas[i] = cross_val_predict(classifier, kfold, X, y)\n",
    "    print(f\"Accuracy of {name}: {accuracy_score(actual, predictions[i])}\")\n",
    "\n",
    "predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting(predicted_probas : np.array) -> np.array:\n",
    "\n",
    "    no_voters = predicted_probas.shape[0]\n",
    "    no_rows = predicted_probas.shape[1]\n",
    "    no_cols = predicted_probas.shape[2]\n",
    "    \n",
    "    sv_predicted_proba = np.empty(shape=(no_rows, no_cols))\n",
    "    sv_predicted_proba.fill(0)\n",
    "    \n",
    "    for i in range(0, no_cols - 1):\n",
    "        for j in range(0, no_voters):\n",
    "            sv_predicted_proba[:, i] += predicted_probas[j][:, i]\n",
    "        sv_predicted_proba[:, i] /= no_voters\n",
    "    \n",
    "    sv_predicted_proba[:,-1] = 1 - sv_predicted_proba.sum(axis=1)\n",
    "    sv_predicted = sv_predicted_proba.argmax(axis=1)\n",
    "    \n",
    "    return sv_predicted_proba, sv_predicted\n",
    "\n",
    "def hard_voting(predictions : np.array) -> np.array:\n",
    "    return [mode(v) for v in predictions.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Soft Voting: 0.8868\n",
      "Accuracy of Hard Voting: 0.881\n"
     ]
    }
   ],
   "source": [
    "predicted_probas = np.array(predicted_probas)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "sv_predicted_proba, sv_predictions = soft_voting(predicted_probas)\n",
    "hv_predictions = hard_voting(predictions)\n",
    "\n",
    "print(f\"Accuracy of Soft Voting: {accuracy_score(actual, sv_predictions)}\")\n",
    "print(f\"Accuracy of Hard Voting: {accuracy_score(actual, hv_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we need to understand the ``predicted_probas`` array by taking a look at its shape ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 3 dimensional array. The first dimension is the number of classification algorithms, the second the number of rows in the data that predictions are held for and the third is the number of classes. Remember this is not a binary classification, we set the number of classes to 3 when the data was generated.\n",
    "\n",
    "This sets things up to set the 3 variables for the number of voters, rows and columns which is not strictly speaking necessary but it does make the subsequent code more readable ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 10000 3\n"
     ]
    }
   ],
   "source": [
    "no_voters = predicted_probas.shape[0]\n",
    "no_rows = predicted_probas.shape[1]\n",
    "no_cols = predicted_probas.shape[2]\n",
    "\n",
    "print(no_voters, no_rows, no_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remind ourselves what the ``predicted_proba`` property looks like for a scikit-learn classifier.\n",
    "\n",
    "The logistic regression ``predicted_proba`` property contains an array with 10,000 rows and 3 columns. It has one row for each data point and one column for each of the classifications, remembering that this is not a binary classification, our classification can have 3 values ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3353\n",
       "1    3330\n",
       "2    3317\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[TARGET_NAME].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44910829, 0.24542133, 0.30547038],\n",
       "       [0.3697233 , 0.34403066, 0.28624605],\n",
       "       [0.26560338, 0.43007386, 0.30432276],\n",
       "       ...,\n",
       "       [0.38909524, 0.26281518, 0.34808958],\n",
       "       [0.15035778, 0.03440465, 0.81523757],\n",
       "       [0.02246867, 0.84043645, 0.13709489]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_predicted_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the first row of data the logistic regression has predicted a 44.9% probability of the row belonging to class 0, a 24.5% probability of class 1 and a 30.5% probability of class 2. This means that the data in the first row is predicted as belonging to class 0 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing the code in the helpder function does is to create an empty array that is going to hold the result of the soft voting. This needs to have 10,000 rows (one for every data point) and three columns (one for the probability of the first class, one for the second and one for the third) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_predicted_proba = np.empty(shape=(no_rows, no_cols))\n",
    "sv_predicted_proba.fill(0)\n",
    "\n",
    "print(sv_predicted_proba.shape)\n",
    "sv_predicted_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next for the main iteration. It will iterate over all the columns except the last one. This seems a bit strange as we need a probability for all of the classifiers but hold on for a bit and we will come back to that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "print(*range(0, no_cols - 1)) # see https://stackoverflow.com/questions/18424899/print-range-of-numbers-on-same-line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the whole iteration ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, no_cols - 1):\n",
    "    for j in range(0, no_voters):\n",
    "        sv_predicted_proba[:, i] += predicted_probas[j][:, i]\n",
    "    sv_predicted_proba[:, i] /= no_voters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are iterating over every column / classifier except the last one. For each column we are adding the predicted probability of each of our algorithms together and dividing by the number of algorithms. When the iteration completes, the result looks like this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18537103, 0.01361911, 0.        ],\n",
       "       [0.69101244, 0.07062499, 0.        ],\n",
       "       [0.50513057, 0.09451146, 0.        ],\n",
       "       ...,\n",
       "       [0.57736211, 0.05994137, 0.        ],\n",
       "       [0.09022216, 0.01052167, 0.        ],\n",
       "       [0.02026342, 0.96622669, 0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_predicted_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final column / classification is blank where as if we had iterated over all of the columns / classifiers it would have been populated. The problem is that had we done so the sum of each row would not always be exactly one due to rounding errors.\n",
    "\n",
    "Therefore, rather than using the iteration to set all three columns / classifiers an additional line of code sets the third one to be 1 minus the sum of the others ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18537103, 0.01361911, 0.80100986],\n",
       "       [0.69101244, 0.07062499, 0.23836257],\n",
       "       [0.50513057, 0.09451146, 0.40035797],\n",
       "       ...,\n",
       "       [0.57736211, 0.05994137, 0.36269652],\n",
       "       [0.09022216, 0.01052167, 0.89925617],\n",
       "       [0.02026342, 0.96622669, 0.01350989]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_predicted_proba[:,-1] = 1 - sv_predicted_proba.sum(axis=1)\n",
    "sv_predicted_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! The ``sv_predicted_proba`` array contains a soft-voted version of the probabilities for each class based on the average of the ``predicted_probas`` property from each of the contributing algorithms,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still need to know the class prediction (i.e. for each row did the soft vote predict class 0, 1 or 2). Fortunately the wonderful ``argmax`` function in the ``numpy`` library enables to do this in a single line of code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_predicted = sv_predicted_proba.argmax(axis=1)\n",
    "sv_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``argmax`` function simply picks the index of the highest value in an array along the axis specified in the ``axis`` parameter, so it picks 2 for the first row, 0 for the second, zero for the third etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have hand crafted the predicted probabilities and the predicted classes using the soft voting algorithm and in the process of writing the code from scratch we have attained a full understanding of exactly how soft voting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard voting is subtly different. Whereas soft voting averages the probability, hard voting picks the class that the majority of the algorithms voted for, for example ...\n",
    "\n",
    "- Class 2, Class 2, Class 0 = Class 2\n",
    "- Class 1, Class 1, Class 2 = Class 1\n",
    "- Class 0, Class 0, Class 0 = Class 0\n",
    "- Class 0, Class 1, Class 2 = Class 0 (actually, it would not matter which one is picked here as they have one vote each)\n",
    "\n",
    "The hard voting algorithm can be implemented in a single line of code using Python list comprehension and ``numpy`` array functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_predicted = [mode(v) for v in predictions.T] # Single line of code to implement the hard voting algorithm ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``predictions.T`` syntax just transposes the array of arrays so that instead of 3 rows and 10,000 columns it is 10,000 rowns and 3 columns ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, ..., 0, 2, 1],\n",
       "       [2, 0, 2, ..., 0, 2, 1],\n",
       "       [2, 0, 0, ..., 0, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2],\n",
       "       [0, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [2, 2, 2],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.T.shape)\n",
    "predictions.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list comprehension then effectively takes each element (row) and applies ``statistics.mode`` to it, thereby selecting the classification that received the most votes from the algorithms ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hv_predicted) # Shows the result for the 1st 3 and last 3 rows as displayed in the previous code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a full understanding of the helpder functions and a deep understanding of how soft and hard voting works those helpder functions can be re-used to generate the results.\n",
    "\n",
    "Note that through trial-and-error I found that the logistic regression has low accuracy for this dataset, hence I have excluded it in the final run to help illustrate the point that soft and hard voting genuinely do improve algorithm accuracy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forrest: 0.8742\n",
      "Accuracy of XG Boost: 0.8838\n",
      "Accuracy of Extra Random Trees: 0.8754\n",
      "Accuracy of Soft Voting: 0.8868\n",
      "Accuracy of Hard Voting: 0.881\n"
     ]
    }
   ],
   "source": [
    "for i, (name, classifier) in enumerate(classifiers.items()):\n",
    "    print(f\"Accuracy of {name}: {accuracy_score(actual, predictions[i])}\")\n",
    "\n",
    "print(f\"Accuracy of Soft Voting: {accuracy_score(actual, sv_predicted)}\")\n",
    "print(f\"Accuracy of Hard Voting: {accuracy_score(actual, hv_predicted)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with that understanding we can revert to using the implementations found in the scikit-learn library safe with the knowledge that we fully understand what they are doing and how they work ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n",
      "Wall time: 1min\n",
      "Accuracy of SciKit-Learn Soft Voting: 0.8868\n",
      "Accuracy of SciKit-Learn Hard Voting: 0.881\n"
     ]
    }
   ],
   "source": [
    "estimators = list(classifiers.items())\n",
    "\n",
    "vc_sv = VotingClassifier(estimators=estimators, voting=\"soft\")\n",
    "vc_hv = VotingClassifier(estimators=estimators, voting=\"hard\")\n",
    "\n",
    "%time actual, vc_sv_predicted, vc_sv_predicted_proba = cross_val_predict(vc_sv, kfold, X, y)\n",
    "%time actual, vc_hv_predicted, _ = cross_val_predict(vc_hv, kfold, X, y)\n",
    "\n",
    "print(f\"Accuracy of SciKit-Learn Soft Voting: {accuracy_score(actual, vc_sv_predicted)}\")\n",
    "print(f\"Accuracy of SciKit-Learn Hard Voting: {accuracy_score(actual, vc_hv_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/saurabhshahane/voting-classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
