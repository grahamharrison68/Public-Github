{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE : int = 42\n",
    "N_SAMPLES : int = 10000\n",
    "N_FEATURES : int = 25\n",
    "N_CLASSES : int = 3\n",
    "N_CLUSTERS_PER_CLASS : int = 2\n",
    "    \n",
    "FEATURE_NAME_PREFIX : str = \"Feature\"\n",
    "TARGET_NAME : str = \"Target\"\n",
    "    \n",
    "N_SPLITS : int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_informative = N_CLASSES * N_CLUSTERS_PER_CLASS\n",
    "X, y = make_classification(n_samples=N_SAMPLES, n_features=N_FEATURES, n_classes=N_CLASSES, n_informative=n_informative, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification_dataframe(n_samples : int = 10000, n_features : int = 25, n_classes : int = 2, n_clusters_per_class : int = 2, feature_name_prefix : str = \"Feature\", target_name : str = \"Target\", random_state : int = 42) -> pd.DataFrame:\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative = n_classes * n_clusters_per_class, random_state=RANDOM_STATE)\n",
    "\n",
    "    feature_names = [feature_name_prefix + \" \" + str(v) for v in np.arange(1, N_FEATURES+1)]\n",
    "    return pd.concat([pd.DataFrame(X, columns=feature_names), pd.DataFrame(y, columns=[target_name])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "      <th>Feature 25</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>-2.025205</td>\n",
       "      <td>-0.089634</td>\n",
       "      <td>-2.833473</td>\n",
       "      <td>0.315723</td>\n",
       "      <td>-0.254786</td>\n",
       "      <td>-1.873841</td>\n",
       "      <td>-1.082022</td>\n",
       "      <td>0.375549</td>\n",
       "      <td>-1.766212</td>\n",
       "      <td>-0.635775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>0.127255</td>\n",
       "      <td>0.964195</td>\n",
       "      <td>-0.570250</td>\n",
       "      <td>-1.121593</td>\n",
       "      <td>-0.859178</td>\n",
       "      <td>-0.390989</td>\n",
       "      <td>-1.916870</td>\n",
       "      <td>-2.367061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>-2.989839</td>\n",
       "      <td>-1.155186</td>\n",
       "      <td>-0.239581</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.410022</td>\n",
       "      <td>-0.348290</td>\n",
       "      <td>-0.758383</td>\n",
       "      <td>1.274005</td>\n",
       "      <td>0.306502</td>\n",
       "      <td>0.080855</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.913510</td>\n",
       "      <td>0.232358</td>\n",
       "      <td>0.684569</td>\n",
       "      <td>-0.683173</td>\n",
       "      <td>0.240665</td>\n",
       "      <td>1.259787</td>\n",
       "      <td>-1.251941</td>\n",
       "      <td>-0.059789</td>\n",
       "      <td>-0.655588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>-1.947663</td>\n",
       "      <td>0.520725</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.019951</td>\n",
       "      <td>1.670833</td>\n",
       "      <td>-0.674143</td>\n",
       "      <td>-0.678134</td>\n",
       "      <td>0.382928</td>\n",
       "      <td>-1.743136</td>\n",
       "      <td>0.115776</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.886197</td>\n",
       "      <td>0.989632</td>\n",
       "      <td>0.165237</td>\n",
       "      <td>1.709442</td>\n",
       "      <td>-1.827470</td>\n",
       "      <td>2.403309</td>\n",
       "      <td>-0.809622</td>\n",
       "      <td>-1.238595</td>\n",
       "      <td>-0.869119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>-0.460435</td>\n",
       "      <td>1.280978</td>\n",
       "      <td>0.722993</td>\n",
       "      <td>0.344352</td>\n",
       "      <td>0.326570</td>\n",
       "      <td>-0.939769</td>\n",
       "      <td>0.130070</td>\n",
       "      <td>0.324532</td>\n",
       "      <td>-0.052836</td>\n",
       "      <td>0.087012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155966</td>\n",
       "      <td>-0.299105</td>\n",
       "      <td>0.262876</td>\n",
       "      <td>0.506887</td>\n",
       "      <td>0.535087</td>\n",
       "      <td>-0.920843</td>\n",
       "      <td>0.187716</td>\n",
       "      <td>0.519180</td>\n",
       "      <td>-0.095456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>-0.053909</td>\n",
       "      <td>1.356961</td>\n",
       "      <td>-1.431071</td>\n",
       "      <td>0.039278</td>\n",
       "      <td>2.191362</td>\n",
       "      <td>-0.511725</td>\n",
       "      <td>0.822338</td>\n",
       "      <td>-0.284092</td>\n",
       "      <td>-0.188173</td>\n",
       "      <td>0.436858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506829</td>\n",
       "      <td>-0.175423</td>\n",
       "      <td>0.582515</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>-0.239184</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.864230</td>\n",
       "      <td>-2.424158</td>\n",
       "      <td>0.160253</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>-1.575353</td>\n",
       "      <td>1.720075</td>\n",
       "      <td>2.444045</td>\n",
       "      <td>-1.141698</td>\n",
       "      <td>-1.363971</td>\n",
       "      <td>0.411163</td>\n",
       "      <td>-2.059946</td>\n",
       "      <td>-0.468634</td>\n",
       "      <td>-1.369811</td>\n",
       "      <td>0.964523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051880</td>\n",
       "      <td>1.075895</td>\n",
       "      <td>-0.197272</td>\n",
       "      <td>0.786871</td>\n",
       "      <td>0.329109</td>\n",
       "      <td>-0.296101</td>\n",
       "      <td>-0.708101</td>\n",
       "      <td>0.328490</td>\n",
       "      <td>-0.901907</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8829</th>\n",
       "      <td>-0.834904</td>\n",
       "      <td>-0.187435</td>\n",
       "      <td>-0.687576</td>\n",
       "      <td>0.633883</td>\n",
       "      <td>-1.973791</td>\n",
       "      <td>-0.560825</td>\n",
       "      <td>-1.535265</td>\n",
       "      <td>-0.376026</td>\n",
       "      <td>1.748435</td>\n",
       "      <td>2.627702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330470</td>\n",
       "      <td>-0.571012</td>\n",
       "      <td>-0.993603</td>\n",
       "      <td>-0.033413</td>\n",
       "      <td>-0.468113</td>\n",
       "      <td>-1.066455</td>\n",
       "      <td>3.255038</td>\n",
       "      <td>0.450539</td>\n",
       "      <td>0.964673</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>-1.791395</td>\n",
       "      <td>-0.823484</td>\n",
       "      <td>2.062622</td>\n",
       "      <td>-2.142280</td>\n",
       "      <td>1.404236</td>\n",
       "      <td>-0.418201</td>\n",
       "      <td>-0.492328</td>\n",
       "      <td>-2.349094</td>\n",
       "      <td>-0.046197</td>\n",
       "      <td>-0.284203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492133</td>\n",
       "      <td>-0.662076</td>\n",
       "      <td>0.452899</td>\n",
       "      <td>0.863350</td>\n",
       "      <td>0.269210</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>-0.822121</td>\n",
       "      <td>-0.710431</td>\n",
       "      <td>-0.187985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>-0.667945</td>\n",
       "      <td>-0.325658</td>\n",
       "      <td>1.083363</td>\n",
       "      <td>0.573356</td>\n",
       "      <td>-0.646951</td>\n",
       "      <td>0.219695</td>\n",
       "      <td>-0.782316</td>\n",
       "      <td>-2.298399</td>\n",
       "      <td>0.856737</td>\n",
       "      <td>0.587982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581827</td>\n",
       "      <td>-0.158002</td>\n",
       "      <td>0.493709</td>\n",
       "      <td>0.590627</td>\n",
       "      <td>0.100678</td>\n",
       "      <td>1.107653</td>\n",
       "      <td>2.093519</td>\n",
       "      <td>1.080767</td>\n",
       "      <td>1.413545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-2.311731</td>\n",
       "      <td>1.759782</td>\n",
       "      <td>-0.157885</td>\n",
       "      <td>-0.964555</td>\n",
       "      <td>0.781729</td>\n",
       "      <td>0.370209</td>\n",
       "      <td>-2.711403</td>\n",
       "      <td>-1.182277</td>\n",
       "      <td>-0.785427</td>\n",
       "      <td>2.392231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352044</td>\n",
       "      <td>-0.624004</td>\n",
       "      <td>-0.850969</td>\n",
       "      <td>1.036668</td>\n",
       "      <td>-1.668777</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>-1.338617</td>\n",
       "      <td>1.536833</td>\n",
       "      <td>-0.556903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>-1.586262</td>\n",
       "      <td>-1.257721</td>\n",
       "      <td>0.337178</td>\n",
       "      <td>-2.721152</td>\n",
       "      <td>0.505817</td>\n",
       "      <td>-1.980687</td>\n",
       "      <td>2.158672</td>\n",
       "      <td>-1.412901</td>\n",
       "      <td>-0.047563</td>\n",
       "      <td>-0.134704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593096</td>\n",
       "      <td>-0.654080</td>\n",
       "      <td>0.490445</td>\n",
       "      <td>0.279981</td>\n",
       "      <td>0.255691</td>\n",
       "      <td>-0.917503</td>\n",
       "      <td>2.031639</td>\n",
       "      <td>-0.455344</td>\n",
       "      <td>0.221627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-1.665737</td>\n",
       "      <td>1.786858</td>\n",
       "      <td>1.096764</td>\n",
       "      <td>0.415518</td>\n",
       "      <td>-1.027915</td>\n",
       "      <td>-0.235249</td>\n",
       "      <td>1.118445</td>\n",
       "      <td>-0.633188</td>\n",
       "      <td>-0.700124</td>\n",
       "      <td>1.213371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912552</td>\n",
       "      <td>-1.167919</td>\n",
       "      <td>-1.522784</td>\n",
       "      <td>0.980845</td>\n",
       "      <td>-0.796538</td>\n",
       "      <td>-0.957670</td>\n",
       "      <td>-1.517241</td>\n",
       "      <td>2.255245</td>\n",
       "      <td>-0.458244</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1.683020</td>\n",
       "      <td>2.553502</td>\n",
       "      <td>1.012461</td>\n",
       "      <td>-0.427417</td>\n",
       "      <td>0.820408</td>\n",
       "      <td>1.790936</td>\n",
       "      <td>-0.998594</td>\n",
       "      <td>1.437156</td>\n",
       "      <td>0.598655</td>\n",
       "      <td>1.046300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.614912</td>\n",
       "      <td>-0.412436</td>\n",
       "      <td>-0.412686</td>\n",
       "      <td>-0.658291</td>\n",
       "      <td>-1.039798</td>\n",
       "      <td>-1.063638</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>0.486463</td>\n",
       "      <td>-0.894321</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>1.643030</td>\n",
       "      <td>1.940539</td>\n",
       "      <td>0.474583</td>\n",
       "      <td>0.638301</td>\n",
       "      <td>0.427153</td>\n",
       "      <td>-1.489614</td>\n",
       "      <td>-0.565465</td>\n",
       "      <td>-1.686859</td>\n",
       "      <td>-1.525171</td>\n",
       "      <td>0.390825</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619692</td>\n",
       "      <td>-0.810523</td>\n",
       "      <td>-0.728254</td>\n",
       "      <td>1.227396</td>\n",
       "      <td>0.895177</td>\n",
       "      <td>-0.510720</td>\n",
       "      <td>3.651149</td>\n",
       "      <td>0.293225</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9529</th>\n",
       "      <td>-1.556052</td>\n",
       "      <td>1.510865</td>\n",
       "      <td>-0.230320</td>\n",
       "      <td>-0.575865</td>\n",
       "      <td>0.974119</td>\n",
       "      <td>0.933166</td>\n",
       "      <td>-0.665065</td>\n",
       "      <td>-0.699268</td>\n",
       "      <td>0.430376</td>\n",
       "      <td>1.381455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676305</td>\n",
       "      <td>-2.038806</td>\n",
       "      <td>-2.354190</td>\n",
       "      <td>-0.390374</td>\n",
       "      <td>0.234016</td>\n",
       "      <td>-0.194457</td>\n",
       "      <td>-1.040316</td>\n",
       "      <td>1.317683</td>\n",
       "      <td>-1.208408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6384</th>\n",
       "      <td>-0.759551</td>\n",
       "      <td>-1.403328</td>\n",
       "      <td>0.872763</td>\n",
       "      <td>-0.921916</td>\n",
       "      <td>-0.685490</td>\n",
       "      <td>-0.311958</td>\n",
       "      <td>0.188483</td>\n",
       "      <td>-0.056606</td>\n",
       "      <td>0.062169</td>\n",
       "      <td>1.299235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>-1.851191</td>\n",
       "      <td>1.160863</td>\n",
       "      <td>0.555677</td>\n",
       "      <td>-0.329880</td>\n",
       "      <td>-0.348243</td>\n",
       "      <td>2.053304</td>\n",
       "      <td>0.511957</td>\n",
       "      <td>1.050907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824</th>\n",
       "      <td>-1.710643</td>\n",
       "      <td>0.728005</td>\n",
       "      <td>-0.008566</td>\n",
       "      <td>1.163424</td>\n",
       "      <td>-2.730729</td>\n",
       "      <td>-0.012596</td>\n",
       "      <td>0.280339</td>\n",
       "      <td>0.941716</td>\n",
       "      <td>-0.621804</td>\n",
       "      <td>-1.172188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305149</td>\n",
       "      <td>0.628988</td>\n",
       "      <td>0.679999</td>\n",
       "      <td>-0.549533</td>\n",
       "      <td>0.891140</td>\n",
       "      <td>-1.051737</td>\n",
       "      <td>2.754586</td>\n",
       "      <td>-0.343649</td>\n",
       "      <td>-0.517088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9779</th>\n",
       "      <td>2.044599</td>\n",
       "      <td>-1.602232</td>\n",
       "      <td>0.374169</td>\n",
       "      <td>0.799452</td>\n",
       "      <td>-1.068989</td>\n",
       "      <td>0.342867</td>\n",
       "      <td>-0.191069</td>\n",
       "      <td>0.044352</td>\n",
       "      <td>-0.908334</td>\n",
       "      <td>-0.472774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179851</td>\n",
       "      <td>0.584845</td>\n",
       "      <td>-0.069943</td>\n",
       "      <td>-0.485961</td>\n",
       "      <td>-0.975222</td>\n",
       "      <td>-0.894740</td>\n",
       "      <td>2.977268</td>\n",
       "      <td>-0.680148</td>\n",
       "      <td>1.917974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>2.304078</td>\n",
       "      <td>-0.381215</td>\n",
       "      <td>-0.005854</td>\n",
       "      <td>0.448535</td>\n",
       "      <td>-0.688349</td>\n",
       "      <td>0.714526</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.608520</td>\n",
       "      <td>0.487476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444018</td>\n",
       "      <td>-1.179971</td>\n",
       "      <td>0.440655</td>\n",
       "      <td>-0.207121</td>\n",
       "      <td>-1.102645</td>\n",
       "      <td>-1.468015</td>\n",
       "      <td>4.752621</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>1.473369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>-3.615533</td>\n",
       "      <td>0.118254</td>\n",
       "      <td>-1.637049</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>2.894535</td>\n",
       "      <td>-0.708038</td>\n",
       "      <td>-0.496120</td>\n",
       "      <td>0.980033</td>\n",
       "      <td>0.158456</td>\n",
       "      <td>0.725948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202847</td>\n",
       "      <td>-0.227342</td>\n",
       "      <td>-1.295629</td>\n",
       "      <td>-0.608249</td>\n",
       "      <td>-1.766336</td>\n",
       "      <td>0.756541</td>\n",
       "      <td>-6.107676</td>\n",
       "      <td>-0.251499</td>\n",
       "      <td>-0.296910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  Feature 6  \\\n",
       "9254  -2.025205  -0.089634  -2.833473   0.315723  -0.254786  -1.873841   \n",
       "1561  -2.989839  -1.155186  -0.239581   0.043799   0.410022  -0.348290   \n",
       "1670  -1.947663   0.520725   0.106356   0.019951   1.670833  -0.674143   \n",
       "6087  -0.460435   1.280978   0.722993   0.344352   0.326570  -0.939769   \n",
       "6669  -0.053909   1.356961  -1.431071   0.039278   2.191362  -0.511725   \n",
       "5933  -1.575353   1.720075   2.444045  -1.141698  -1.363971   0.411163   \n",
       "8829  -0.834904  -0.187435  -0.687576   0.633883  -1.973791  -0.560825   \n",
       "7945  -1.791395  -0.823484   2.062622  -2.142280   1.404236  -0.418201   \n",
       "3508  -0.667945  -0.325658   1.083363   0.573356  -0.646951   0.219695   \n",
       "2002  -2.311731   1.759782  -0.157885  -0.964555   0.781729   0.370209   \n",
       "5866  -1.586262  -1.257721   0.337178  -2.721152   0.505817  -1.980687   \n",
       "168   -1.665737   1.786858   1.096764   0.415518  -1.027915  -0.235249   \n",
       "2588   1.683020   2.553502   1.012461  -0.427417   0.820408   1.790936   \n",
       "5905   1.643030   1.940539   0.474583   0.638301   0.427153  -1.489614   \n",
       "9529  -1.556052   1.510865  -0.230320  -0.575865   0.974119   0.933166   \n",
       "6384  -0.759551  -1.403328   0.872763  -0.921916  -0.685490  -0.311958   \n",
       "7824  -1.710643   0.728005  -0.008566   1.163424  -2.730729  -0.012596   \n",
       "9779   2.044599  -1.602232   0.374169   0.799452  -1.068989   0.342867   \n",
       "4629   2.304078  -0.381215  -0.005854   0.448535  -0.688349   0.714526   \n",
       "8560  -3.615533   0.118254  -1.637049   0.048214   2.894535  -0.708038   \n",
       "\n",
       "      Feature 7  Feature 8  Feature 9  Feature 10  ...  Feature 17  \\\n",
       "9254  -1.082022   0.375549  -1.766212   -0.635775  ...    0.450156   \n",
       "1561  -0.758383   1.274005   0.306502    0.080855  ...   -1.913510   \n",
       "1670  -0.678134   0.382928  -1.743136    0.115776  ...   -1.886197   \n",
       "6087   0.130070   0.324532  -0.052836    0.087012  ...   -0.155966   \n",
       "6669   0.822338  -0.284092  -0.188173    0.436858  ...    0.506829   \n",
       "5933  -2.059946  -0.468634  -1.369811    0.964523  ...    0.051880   \n",
       "8829  -1.535265  -0.376026   1.748435    2.627702  ...    0.330470   \n",
       "7945  -0.492328  -2.349094  -0.046197   -0.284203  ...    0.492133   \n",
       "3508  -0.782316  -2.298399   0.856737    0.587982  ...    0.581827   \n",
       "2002  -2.711403  -1.182277  -0.785427    2.392231  ...    0.352044   \n",
       "5866   2.158672  -1.412901  -0.047563   -0.134704  ...   -0.593096   \n",
       "168    1.118445  -0.633188  -0.700124    1.213371  ...    0.912552   \n",
       "2588  -0.998594   1.437156   0.598655    1.046300  ...   -0.614912   \n",
       "5905  -0.565465  -1.686859  -1.525171    0.390825  ...    1.619692   \n",
       "9529  -0.665065  -0.699268   0.430376    1.381455  ...    0.676305   \n",
       "6384   0.188483  -0.056606   0.062169    1.299235  ...   -0.008287   \n",
       "7824   0.280339   0.941716  -0.621804   -1.172188  ...    0.305149   \n",
       "9779  -0.191069   0.044352  -0.908334   -0.472774  ...   -0.179851   \n",
       "4629  -0.045112   0.038541   0.608520    0.487476  ...   -0.444018   \n",
       "8560  -0.496120   0.980033   0.158456    0.725948  ...   -0.202847   \n",
       "\n",
       "      Feature 18  Feature 19  Feature 20  Feature 21  Feature 22  Feature 23  \\\n",
       "9254    0.127255    0.964195   -0.570250   -1.121593   -0.859178   -0.390989   \n",
       "1561    0.232358    0.684569   -0.683173    0.240665    1.259787   -1.251941   \n",
       "1670    0.989632    0.165237    1.709442   -1.827470    2.403309   -0.809622   \n",
       "6087   -0.299105    0.262876    0.506887    0.535087   -0.920843    0.187716   \n",
       "6669   -0.175423    0.582515    0.030940   -0.239184    0.015029    0.864230   \n",
       "5933    1.075895   -0.197272    0.786871    0.329109   -0.296101   -0.708101   \n",
       "8829   -0.571012   -0.993603   -0.033413   -0.468113   -1.066455    3.255038   \n",
       "7945   -0.662076    0.452899    0.863350    0.269210    0.020738   -0.822121   \n",
       "3508   -0.158002    0.493709    0.590627    0.100678    1.107653    2.093519   \n",
       "2002   -0.624004   -0.850969    1.036668   -1.668777    0.943739   -1.338617   \n",
       "5866   -0.654080    0.490445    0.279981    0.255691   -0.917503    2.031639   \n",
       "168    -1.167919   -1.522784    0.980845   -0.796538   -0.957670   -1.517241   \n",
       "2588   -0.412436   -0.412686   -0.658291   -1.039798   -1.063638   -0.757295   \n",
       "5905   -0.810523   -0.728254    1.227396    0.895177   -0.510720    3.651149   \n",
       "9529   -2.038806   -2.354190   -0.390374    0.234016   -0.194457   -1.040316   \n",
       "6384   -1.851191    1.160863    0.555677   -0.329880   -0.348243    2.053304   \n",
       "7824    0.628988    0.679999   -0.549533    0.891140   -1.051737    2.754586   \n",
       "9779    0.584845   -0.069943   -0.485961   -0.975222   -0.894740    2.977268   \n",
       "4629   -1.179971    0.440655   -0.207121   -1.102645   -1.468015    4.752621   \n",
       "8560   -0.227342   -1.295629   -0.608249   -1.766336    0.756541   -6.107676   \n",
       "\n",
       "      Feature 24  Feature 25  Target  \n",
       "9254   -1.916870   -2.367061       0  \n",
       "1561   -0.059789   -0.655588       0  \n",
       "1670   -1.238595   -0.869119       0  \n",
       "6087    0.519180   -0.095456       0  \n",
       "6669   -2.424158    0.160253       0  \n",
       "5933    0.328490   -0.901907       2  \n",
       "8829    0.450539    0.964673       1  \n",
       "7945   -0.710431   -0.187985       0  \n",
       "3508    1.080767    1.413545       0  \n",
       "2002    1.536833   -0.556903       0  \n",
       "5866   -0.455344    0.221627       0  \n",
       "168     2.255245   -0.458244       2  \n",
       "2588    0.486463   -0.894321       2  \n",
       "5905    0.293225    0.007703       1  \n",
       "9529    1.317683   -1.208408       0  \n",
       "6384    0.511957    1.050907       0  \n",
       "7824   -0.343649   -0.517088       1  \n",
       "9779   -0.680148    1.917974       1  \n",
       "4629   -0.002451    1.473369       1  \n",
       "8560   -0.251499   -0.296910       0  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = make_classification_dataframe(n_samples=N_SAMPLES, n_features=N_FEATURES, n_classes=N_CLASSES, n_clusters_per_class=N_CLUSTERS_PER_CLASS, feature_name_prefix=FEATURE_NAME_PREFIX, target_name=TARGET_NAME, random_state=RANDOM_STATE)\n",
    "\n",
    "df_data_train, df_data_val = train_test_split(df_data, test_size=0.2, random_state=RANDOM_STATE)\n",
    "df_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict(model, kfold : KFold, X : np.array, y : np.array, target_type : type = int) -> Tuple[np.array, np.array, np.array]:\n",
    "\n",
    "    model_ = cp.deepcopy(model)\n",
    "    \n",
    "    actual_classes = np.array([])\n",
    "    predicted_classes = np.array([])\n",
    "    predicted_proba = np.array([])\n",
    "\n",
    "    splits = kfold.split(X)\n",
    "    \n",
    "    for train_ndx, test_ndx in splits:\n",
    "\n",
    "        train_X, train_y, test_X, test_y = X[train_ndx], y[train_ndx], X[test_ndx], y[test_ndx]\n",
    "\n",
    "        actual_classes = np.append(actual_classes, test_y)\n",
    "\n",
    "        model_.fit(train_X, train_y)\n",
    "        predicted_classes = np.append(predicted_classes, model_.predict(test_X))\n",
    "        predicted_proba = np.append(predicted_proba, model_.predict_proba(test_X))\n",
    "\n",
    "    return actual_classes.astype(target_type), predicted_classes.astype(target_type), predicted_proba.reshape(X.shape[0], len(np.unique(y))) # Reshape to the number of rows in the source features and the number of unique classes that appear in the target. For example 10,000 data points with y = 0 or 1 will have produced an array (20000,) in shape that needs to be reshaped to (10000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=N_SPLITS, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "xg = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data_train.drop([TARGET_NAME], axis=1)\n",
    "y = df_data_train[TARGET_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_actual, lr_predicted, lr_predicted_proba = cross_val_predict(lr, kfold, X.to_numpy(), y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_actual, rf_predicted, rf_predicted_proba = cross_val_predict(rf, kfold, X.to_numpy(), y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xg_actual, xg_predicted, xg_predicted_proba = cross_val_predict(xg, kfold, X.to_numpy(), y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.18407243, -0.56677271, -0.61729971],\n",
       "       [ 0.81343434, -0.16398338, -0.64945096],\n",
       "       [ 1.61010708, -1.12451957, -0.48558751],\n",
       "       ...,\n",
       "       [ 1.24665246, -1.44689942,  0.20024696],\n",
       "       [-0.24804334,  0.23050748,  0.01753586],\n",
       "       [-0.10821081,  1.00913904, -0.90092823]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting(predicted_probas : np.array) -> np.array:\n",
    "    \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            predicted_probas (np.array): [description]\n",
    "\n",
    "        Returns:\n",
    "            np.array: [description]\n",
    "        \"\"\"\n",
    "    \n",
    "    no_voters = predicted_probas.shape[0]\n",
    "    no_rows = predicted_probas.shape[1]\n",
    "    no_cols = predicted_probas.shape[2]\n",
    "    \n",
    "    soft_voting_probas = np.empty(shape=(no_rows, no_cols))\n",
    "    soft_voting_probas.fill(0)\n",
    "    \n",
    "    for i in range(0, no_cols - 1):\n",
    "        for j in range(0, no_voters):\n",
    "            soft_voting_probas[:, i] += predicted_probas[j][:, i]\n",
    "        soft_voting_probas[:, i] /= no_voters\n",
    "    \n",
    "    soft_voting_probas[:,-1] = 1 - soft_voting_probas.sum(axis=1)\n",
    "    \n",
    "    return soft_voting_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probas = np.array([lr_predicted_proba, rf_predicted_proba, xg_predicted_proba])\n",
    "soft_voting_probas = soft_voting(predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06155353, 0.92809167, 0.01035481],\n",
       "       [0.75616003, 0.16764789, 0.07619208],\n",
       "       [0.66218234, 0.05924803, 0.27856963],\n",
       "       ...,\n",
       "       [0.32121056, 0.56244027, 0.11634917],\n",
       "       [0.18353267, 0.03270084, 0.7837665 ],\n",
       "       [0.58621584, 0.0869719 , 0.32681226]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_voting_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12441844, 0.84454805, 0.03103352],\n",
       "       [0.56246366, 0.27943497, 0.15810137],\n",
       "       [0.46456839, 0.15673804, 0.37869357],\n",
       "       ...,\n",
       "       [0.40873642, 0.38146858, 0.209795  ],\n",
       "       [0.23654082, 0.08590648, 0.67755271],\n",
       "       [0.42188846, 0.22544336, 0.35266818]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06, 0.94, 0.  ],\n",
       "       [0.71, 0.22, 0.07],\n",
       "       [0.76, 0.02, 0.22],\n",
       "       ...,\n",
       "       [0.42, 0.49, 0.09],\n",
       "       [0.19, 0.01, 0.8 ],\n",
       "       [0.65, 0.03, 0.32]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.42143142e-04, 9.99726951e-01, 3.08621820e-05],\n",
       "       [9.96016443e-01, 3.50869191e-03, 4.74897592e-04],\n",
       "       [7.61978626e-01, 1.00605446e-03, 2.37015381e-01],\n",
       "       ...,\n",
       "       [1.34895250e-01, 8.15852225e-01, 4.92525101e-02],\n",
       "       [1.24057181e-01, 2.19603023e-03, 8.73746812e-01],\n",
       "       [6.86759055e-01, 5.47233177e-03, 3.07768643e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061553526266699236"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lr_predicted_proba[0][0] + rf_predicted_proba[0][0] + xg_predicted_proba[0][0]) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.061553526266699236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "for i in range(0, soft_voting_probas.shape[1]):\n",
    "    tot += soft_voting_probas[0][i]\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_voting_probas[0][0] + soft_voting_probas[0][1] + + soft_voting_probas[0][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
