{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/ensemble-learning-stacking-blending-voting-b37737c4f483\n",
    "\n",
    "https://github.com/FernandoLpz/Stacking-Blending-Voting-Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.k = 5\n",
    "\n",
    "    def load_data(self):\n",
    "        x, y = load_breast_cancer(return_X_y=True)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.3, random_state=23)\n",
    "\n",
    "    def StackingClassifier(self):\n",
    "\n",
    "        # Define weak learners\n",
    "        weak_learners = [('dt', DecisionTreeClassifier()),\n",
    "                         ('knn', KNeighborsClassifier()),\n",
    "                         ('rf', RandomForestClassifier()),\n",
    "                         ('gb', GradientBoostingClassifier()),\n",
    "                         ('gn', GaussianNB())]\n",
    "\n",
    "        # Final learner or meta model\n",
    "        final_learner = LogisticRegression()\n",
    "\n",
    "        train_meta_model = None\n",
    "        test_meta_model = None\n",
    "\n",
    "        # Start stacking\n",
    "        for clf_id, clf in weak_learners:\n",
    "            # Predictions for each classifier based on k-fold\n",
    "            predictions_clf = self.k_fold_cross_validation(clf)\n",
    "\n",
    "            # Predictions for test set for each classifier based on train of level 0\n",
    "            test_predictions_clf = self.train_level_0(clf)\n",
    "\n",
    "            # Stack predictions which will form \n",
    "            # the input data for the data model\n",
    "            if isinstance(train_meta_model, np.ndarray):\n",
    "                train_meta_model = np.vstack((train_meta_model, predictions_clf))\n",
    "            else:\n",
    "                train_meta_model = predictions_clf\n",
    "\n",
    "            # Stack predictions from test set\n",
    "            # which will form test data for meta model\n",
    "            if isinstance(test_meta_model, np.ndarray):\n",
    "                test_meta_model = np.vstack((test_meta_model, test_predictions_clf))\n",
    "            else:\n",
    "                test_meta_model = test_predictions_clf\n",
    "\n",
    "        # Transpose train_meta_model\n",
    "        train_meta_model = train_meta_model.T\n",
    "\n",
    "        # Transpose test_meta_model\n",
    "        test_meta_model = test_meta_model.T\n",
    "\n",
    "        # Training level 1\n",
    "        self.train_level_1(final_learner, train_meta_model, test_meta_model)\n",
    "\n",
    "    def k_fold_cross_validation(self, clf):\n",
    "\n",
    "        predictions_clf = None\n",
    "\n",
    "        # Number of samples per fold\n",
    "        batch_size = int(len(self.x_train) / self.k)\n",
    "\n",
    "        # Stars k-fold cross validation\n",
    "        for fold in range(self.k):\n",
    "\n",
    "            # Settings for each batch_size\n",
    "            if fold == (self.k - 1):\n",
    "                test = self.x_train[(batch_size * fold):, :]\n",
    "                batch_start = batch_size * fold\n",
    "                batch_finish = self.x_train.shape[0]\n",
    "            else:\n",
    "                test = self.x_train[(batch_size * fold): (batch_size * (fold + 1)), :]\n",
    "                batch_start = batch_size * fold\n",
    "                batch_finish = batch_size * (fold + 1)\n",
    "\n",
    "            # test & training samples for each fold iteration\n",
    "            fold_x_test = self.x_train[batch_start:batch_finish, :]\n",
    "            fold_x_train = self.x_train[[index for index in range(self.x_train.shape[0]) if\n",
    "                                         index not in range(batch_start, batch_finish)], :]\n",
    "\n",
    "            # test & training targets for each fold iteration\n",
    "            fold_y_test = self.y_train[batch_start:batch_finish]\n",
    "            fold_y_train = self.y_train[\n",
    "                [index for index in range(self.x_train.shape[0]) if index not in range(batch_start, batch_finish)]]\n",
    "\n",
    "            # Fit current classifier\n",
    "            clf.fit(fold_x_train, fold_y_train)\n",
    "            fold_y_pred = clf.predict(fold_x_test)\n",
    "\n",
    "            # Store predictions for each fold_x_test\n",
    "            if isinstance(predictions_clf, np.ndarray):\n",
    "                predictions_clf = np.concatenate((predictions_clf, fold_y_pred))\n",
    "            else:\n",
    "                predictions_clf = fold_y_pred\n",
    "\n",
    "        return predictions_clf\n",
    "\n",
    "    def train_level_0(self, clf):\n",
    "        # Train in full real training set\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        # Get predictions from full real test set\n",
    "        y_pred = clf.predict(self.x_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train_level_1(self, final_learner, train_meta_model, test_meta_model):\n",
    "        # Train is carried out with final learner or meta model\n",
    "        final_learner.fit(train_meta_model, self.y_train)\n",
    "        # Getting train and test accuracies from meta_model\n",
    "        print(f\"Train accuracy: {final_learner.score(train_meta_model, self.y_train)}\")\n",
    "        print(f\"Test accuracy: {final_learner.score(test_meta_model, self.y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.957286432160804\n",
      "Test accuracy: 0.9766081871345029\n"
     ]
    }
   ],
   "source": [
    "ensemble = Ensemble()\n",
    "ensemble.load_data()\n",
    "ensemble.StackingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(clf):\n",
    "\n",
    "    predictions_clf = None\n",
    "\n",
    "    # Number of samples per fold\n",
    "    batch_size = int(len(x_train) / k)\n",
    "\n",
    "    # Stars k-fold cross validation\n",
    "    for fold in range(k):\n",
    "\n",
    "        # Settings for each batch_size\n",
    "        if fold == (k - 1):\n",
    "            test = x_train[(batch_size * fold):, :]\n",
    "            batch_start = batch_size * fold\n",
    "            batch_finish = x_train.shape[0]\n",
    "        else:\n",
    "            test = x_train[(batch_size * fold): (batch_size * (fold + 1)), :]\n",
    "            batch_start = batch_size * fold\n",
    "            batch_finish = batch_size * (fold + 1)\n",
    "\n",
    "        # test & training samples for each fold iteration\n",
    "        fold_x_test = x_train[batch_start:batch_finish, :]\n",
    "        fold_x_train = x_train[[index for index in range(x_train.shape[0]) if\n",
    "                                     index not in range(batch_start, batch_finish)], :]\n",
    "\n",
    "        # test & training targets for each fold iteration\n",
    "        fold_y_test = y_train[batch_start:batch_finish]\n",
    "        fold_y_train = y_train[\n",
    "            [index for index in range(x_train.shape[0]) if index not in range(batch_start, batch_finish)]]\n",
    "\n",
    "        # Fit current classifier\n",
    "        clf.fit(fold_x_train, fold_y_train)\n",
    "        fold_y_pred = clf.predict(fold_x_test)\n",
    "\n",
    "        # Store predictions for each fold_x_test\n",
    "        if isinstance(predictions_clf, np.ndarray):\n",
    "            predictions_clf = np.concatenate((predictions_clf, fold_y_pred))\n",
    "        else:\n",
    "            predictions_clf = fold_y_pred\n",
    "\n",
    "    return predictions_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_level_0(clf):\n",
    "    # Train in full real training set\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Get predictions from full real test set\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_level_1(final_learner, train_meta_model, test_meta_model):\n",
    "    # Train is carried out with final learner or meta model\n",
    "    final_learner.fit(train_meta_model, y_train)\n",
    "    # Getting train and test accuracies from meta_model\n",
    "    print(f\"Train accuracy: {final_learner.score(train_meta_model, y_train)}\")\n",
    "    print(f\"Test accuracy: {final_learner.score(test_meta_model, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_breast_cancer(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_learners = [('dt', DecisionTreeClassifier()),\n",
    "                 ('knn', KNeighborsClassifier()),\n",
    "                 ('rf', RandomForestClassifier()),\n",
    "                 ('gb', GradientBoostingClassifier()),\n",
    "                 ('gn', GaussianNB())]\n",
    "\n",
    "# Final learner or meta model\n",
    "final_learner = LogisticRegression()\n",
    "\n",
    "train_meta_model = None\n",
    "test_meta_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start stacking\n",
    "for clf_id, clf in weak_learners:\n",
    "    # Predictions for each classifier based on k-fold\n",
    "    predictions_clf = k_fold_cross_validation(clf)\n",
    "\n",
    "    # Predictions for test set for each classifier based on train of level 0\n",
    "    test_predictions_clf = train_level_0(clf)\n",
    "\n",
    "    # Stack predictions which will form \n",
    "    # the input data for the data model\n",
    "    if isinstance(train_meta_model, np.ndarray):\n",
    "        train_meta_model = np.vstack((train_meta_model, predictions_clf))\n",
    "    else:\n",
    "        train_meta_model = predictions_clf\n",
    "\n",
    "    # Stack predictions from test set\n",
    "    # which will form test data for meta model\n",
    "    if isinstance(test_meta_model, np.ndarray):\n",
    "        test_meta_model = np.vstack((test_meta_model, test_predictions_clf))\n",
    "    else:\n",
    "        test_meta_model = test_predictions_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 1, 1],\n",
       "       [1, 0, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 1, ..., 0, 1, 1],\n",
       "       [1, 0, 1, ..., 0, 1, 1],\n",
       "       [1, 1, 0, ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 171)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose train_meta_model\n",
    "train_meta_model = train_meta_model.T\n",
    "\n",
    "# Transpose test_meta_model\n",
    "test_meta_model = test_meta_model.T\n",
    "\n",
    "# Training level 1\n",
    "#train_level_1(final_learner, train_meta_model, test_meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_learner.fit(train_meta_model, y_train)\n",
    "        # Getting train and test accuracies from meta_model\n",
    "print(f\"Train accuracy: {final_learner.score(train_meta_model, self.y_train)}\")\n",
    "print(f\"Test accuracy: {final_learner.score(test_meta_model, self.y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
